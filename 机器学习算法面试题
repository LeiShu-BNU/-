2021.4.6
1，为什么要对特征做归一化 ？
不同特征的数量级（量纲）不同，在进行距离有关的计算时，单位的不同会导致计算结果的不同，尺度大的特征会起决定性作用，而尺度小的特征其作用可能会被忽略。比如特征x1 house price的数量级是10000-100000，而特征x2 面积的数量级是10-1000，特征x3房间数的数量级是1-10。

2，什么是组合特征？如何处理高维组合特征？ 
特征的组合：把多个特征合并成一个特征，如“浴室1面积”、“浴室2面积”、“浴室3面积”可合并成一个“浴室总面积”
或文本特征向量的拼接：直接在向量后面拼接另一个语言模型生成的文本向量（大幅度增加特征数量）
高维组合特征：奇异值分解SVD，起到降维的作用
补充：深度学习推荐系统中各类流行的Embedding方法：https://blog.csdn.net/abcdefg90876/article/details/106464384/


3，请比较欧式距离与曼哈顿距离？
欧式距离是AB两点之间的直线距离
曼哈顿距离是限制只能沿着坐标轴方向行走，从A到B的距离
补充：何时用曼哈顿：https://www.cnblogs.com/Renyi-Fan/p/8137948.html#_label0
导航地图用曼哈顿估计距离、每个维度权重相等，而欧式距离各个维度权重不等。

4，为什么一些场景中使用余弦相似度而不是欧式距离 ？
余弦相似度在高维的情况下依然保持“相同时为1，正交时为0，相反时为-1”的性质。
欧式距离的数值受维度的影响，范围不固定，并且含义也比较模糊。
欧式距离体现数值上的绝对差异，而余弦距离体现方向上的相对差异。

附加题：One-hot的作用是什么？为什么不直接使用数字作为表示
采用哑变量(dummy variables) 对类别进行编码。在类别型特征中，数字不一定表示连续性的数值大小，而是离散的类别编号。如商家编号1、2、3、4.如果当做数字（有序型）处理就有偏差。
因此用one-hot把这个特征拆分成多个小特征，每个特征只有是或否，用1和0表示。

2021.4.7
1.在模型评估过程中，过拟合和欠拟合具体指什么现象？
过拟合：在训练集上效果很好，但是在测试集上效果差。为了得到一致假设而使假设变得过度复杂称为过拟合(overfitting)
欠拟合：训练集和测试集都差。欠拟合是指模型拟合程度不高，数据距离拟合曲线较远，或指模型没有很好地捕捉到数据特征，不能够很好地拟合数据。
2.降低过拟合和欠拟合的方法
降低过拟合：
减少不必要的特征
根据特征贡献程度，删除不重要的稀疏特征
用数据增广产生更多训练数据，防止模型学习到不相干的特征
Early Stopping减少迭代次数、深度模型中使用DROPOUT:
每一批次数据，由于随机性剔除神经元，使得网络具有一定的稀疏性，从而能减轻了不同特征之间的协同效应。而且由于每次被剔除的神经元不同，所以整个网络神经元的参数也只是部分被更新，消除减弱了神经元间的联合适应性，增强了神经网络的泛化能力和鲁棒性。Dropout只在训练时使用，作为一个超参数，然而在测试集时，并不能使用。
减少欠拟合：增加迭代次数、采用验证集、增加新特征、组合特征

3.L1和L2正则先验分别服从什么分布
L1拉普拉斯分布，L2正态分布
L2 regularizer ：使得模型的解偏向于范数较小的 W，通过限制 W 范数的大小实现了对模型空间的限制，从而在一定程度上避免了 overfitting 。不过 ridge regression 并不具有产生稀疏解的能力，得到的系数仍然需要数据中的所有特征才能计算预测结果，从计算量上来说并没有得到改观。
L1 regularizer ：它的优良性质是能产生稀疏性，导致 W 中许多项变成零。 稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。
参考：https://www.cnblogs.com/heguanyou/p/7688344.html
https://www.cnblogs.com/heguanyou/p/7688344.html
那么如何判断什么时候用L1什么时候用L2呢？https://blog.csdn.net/jinping_shi/article/details/52433975


4.对于树形结构为什么不需要归一化？
决策树的学习过程本质上是选择合适的特征，分裂并构建树节点的过程；而分裂节点的标准是由树构建前后的信息增益，信息增益比以及基尼系数等指标决定的。这些指标与当前特征值的大小本身并无关系。
而非用特征形成一个向量互相计算。因此每一个特征是独立的，可解释性也好。

附加题：什么是数据不平衡，如何解决？
在分类任务中：不同类别的训练样本数量差异大。比如正面情感有10000条，负面情感只有5000条。如果把训练集按照原始数据的比例来构建，会赋予样本数量较多的类别更大的权重，影响预测结果。
绝大多数常见的机器学习算法对于不平衡数据集都不能很好地工作。
解决方法：
重新采样训练集
a.       欠采样 –减少丰富类的大小来平衡数据集
b.       过采样 – 增加稀有样本，通过使用重复，自举或合成少数类
设计使用不平衡数据集的模型
a.       在代价函数中惩罚稀有类别的错误分类。

2021.4.9
1.逻辑回归相比线性回归，有何异同？
不同之处：
逻辑回归解决的是分类问题，线性回归解决的是回归问题，这是两者最本质的区别
逻辑回归中因变量是离散的，而线性回归中因变量是连续的这是两者最大的区别
在自变量和超参数确定的情况下逻辑回归可看作广义的线性模型在因变量下服从二元分布的一个特殊情况
使用最小二乘法求解线性回归时我们认为因变量服从正态分布
相同之处：
二者在求解超参数的过程中都使用梯度下降的方法
二者都使用了极大似然估计对训练样本进行建模

2.回归问题常用的性能度量指标
MSE：(y - y')^2取平均
y_preditc=reg.predict(x_test) #reg是训练好的模型
mse_test=np.sum((y_preditc-y_test)**2)/len(y_test) #跟数学公式一样的
RMSE：(y - y')^2取平均,再开根
rmse_test=mse_test ** 0.5
R2：1- mean_squared_error(y_test,y_preditc)/ np.var(y_test)
MAE：|y-y'|的均值
mae_test=np.sum(np.absolute(y_preditc-y_test))/len(y_test)
3.分类问题常用的性能度量指标
acc, precision，recall,f1，P-R曲线，ROC,AUC
4.逻辑回归的损失函数
用sigmoid表示取0和1的概率，把线性回归问题变成分类问题。写作h(x)函数。
当y=0时，cost(h(x),y)=-log(1-h(x))
当y=1时，cost(h(x),y)=-log(h(x))
损失函数就是在0-1分布的基础上取对数然后再取负数。这也好理解，损失函数的要求就是预测结果与真实结果越相近，函数值越小，所以会在前面加上负号。当y=0时，1-p的概率会比较大，在前面加上负号，Cost值就会很小；当y=1时，p的概率会比较大，在前面加上负号，Cost值就会很小。至于取对数，就是跟最大似然函数有关系，取对数不影响原本函数的单调性，而且会放大概率之间的差异，更好的区分各个样本的类别。
https://www.jianshu.com/p/b6bb6c035d8c
5.逻辑回归处理多标签分类问题时，一般怎么做？
one vs rest:: 还是将类别分为两类: 一类是A类，另一类就是其他所有类（把其他所有类当成一类来看）
one vs one将一个多分类问题中的所有类都两两拿出来训练，训练成多个二分类的模型。可以训练出k个二分类的逻辑回归分类器。第i个分类器用以区分每个样本是否可以归类为第i类，训练该分类器的时候，需要把标签重新整理为第i类标签和非第i类标签两类。通过这样的方法，就解决了每个样本可能拥有多个标签的情况。


1.写出全概率公式&贝叶斯公式
贝叶斯公式:
P(A|B)=P(B|A)P(A)/P(B)
全概率公式:
设实验E的样本空间为S，A是S中的一个事件，B1,B2...Bn是S的一种划分，那么P(A) = P(A|B1)P(B1)+ P(A|B2)P(B2)+...+ P(A|Bn)P(Bn)
2.朴素贝叶斯为什么“朴素naive”？
特征的条件独立性假设：预设各个事件是独立的，也就是每一个维度（特征）互相独立，没有互相影响、互为条件
比如一句话里面的每个词出现的概率都是独立的，也就是在语料库中的频率。
（半朴素贝叶斯是适当考虑一部分属性之间的相互依赖信息，其中“独依赖估计”（One-Dependent Estimator，简称ODE）是半朴素分类中最常用的一种策略。）
3.朴素贝叶斯有没有超参数可以调？
朴素贝叶斯是没有超参数可以调的，它不需要调参。
朴素贝叶斯是根据训练集进行分类，分类出来的结果基本上就是确定了的，拉普拉斯估计器不是朴素贝叶斯中的参数，不能通过拉普拉斯估计器来对朴素贝叶斯调参。（拉普拉斯平滑法是朴素贝叶斯中处理零概率问题的一种修正方式。在进行分类的时候，可能会出现某个属性在训练集中没有与某个类同时出现过的情况，如果直接基于朴素贝叶斯分类器的表达式进行计算的话就会出现零概率现象。为了避免其他属性所携带的信息被训练集中未出现过的属性值“抹去”，所以才使用拉普拉斯估计器进行修正。具体的方法是：在分子上加1,对于先验概率，在分母上加上训练集中可能的类别数；对于条件概率，则在分母上加上第i个属性可能的取值数）
4.朴素贝叶斯的工作流程是怎样的？
准备阶段：这个阶段的任务是为朴素贝叶斯分类做必要的准备，主要工作是根据具体情况确定特征属性，并对每个特征属性进行适当划分，去除高度相关性的属性(如果两个属性具有高度相关性的话，那么该属性将会在模型中发挥了2次作用，会使得朴素贝叶斯所预测的结果向该属性所希望的方向偏离，导致分类出现偏差)，然后由人工对一部分待分类项进行分类，形成训练样本集合。这一阶段的输入是所有待分类数据，输出是特征属性和训练样本。(这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响。)
分类器训练阶段：这个阶段的任务就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计，并将结果记录。其输入是特征属性和训练样本，输出是分类器。这一阶段是机械性阶段，根据前面讨论的公式可以由程序自动计算完成。
应用阶段：这个阶段的任务是使用分类器对待分类项进行分类，其输入是分类器和待分类项，输出是待分类项与类别的映射关系。这一阶段也是机械性阶段，由程序完成。
