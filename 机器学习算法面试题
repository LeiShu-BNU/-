2021.4.6
1，为什么要对特征做归一化 ？
不同特征的数量级（量纲）不同，在进行距离有关的计算时，单位的不同会导致计算结果的不同，尺度大的特征会起决定性作用，而尺度小的特征其作用可能会被忽略。比如特征x1 house price的数量级是10000-100000，而特征x2 面积的数量级是10-1000，特征x3房间数的数量级是1-10。

2，什么是组合特征？如何处理高维组合特征？ 
特征的组合：把多个特征合并成一个特征，如“浴室1面积”、“浴室2面积”、“浴室3面积”可合并成一个“浴室总面积”
或文本特征向量的拼接：直接在向量后面拼接另一个语言模型生成的文本向量（大幅度增加特征数量）
高维组合特征：奇异值分解SVD，起到降维的作用
补充：深度学习推荐系统中各类流行的Embedding方法：https://blog.csdn.net/abcdefg90876/article/details/106464384/


3，请比较欧式距离与曼哈顿距离？
欧式距离是AB两点之间的直线距离
曼哈顿距离是限制只能沿着坐标轴方向行走，从A到B的距离
补充：何时用曼哈顿：https://www.cnblogs.com/Renyi-Fan/p/8137948.html#_label0
导航地图用曼哈顿估计距离、每个维度权重相等，而欧式距离各个维度权重不等。

4，为什么一些场景中使用余弦相似度而不是欧式距离 ？
余弦相似度在高维的情况下依然保持“相同时为1，正交时为0，相反时为-1”的性质。
欧式距离的数值受维度的影响，范围不固定，并且含义也比较模糊。
欧式距离体现数值上的绝对差异，而余弦距离体现方向上的相对差异。

附加题：One-hot的作用是什么？为什么不直接使用数字作为表示
采用哑变量(dummy variables) 对类别进行编码。在类别型特征中，数字不一定表示连续性的数值大小，而是离散的类别编号。如商家编号1、2、3、4.如果当做数字（有序型）处理就有偏差。
因此用one-hot把这个特征拆分成多个小特征，每个特征只有是或否，用1和0表示。

2021.4.7
1.在模型评估过程中，过拟合和欠拟合具体指什么现象？
过拟合：在训练集上效果很好，但是在测试集上效果差。为了得到一致假设而使假设变得过度复杂称为过拟合(overfitting)
欠拟合：训练集和测试集都差。欠拟合是指模型拟合程度不高，数据距离拟合曲线较远，或指模型没有很好地捕捉到数据特征，不能够很好地拟合数据。
2.降低过拟合和欠拟合的方法
降低过拟合：Early Stopping减少迭代次数、深度模型中使用DROPOUT:
每一批次数据，由于随机性剔除神经元，使得网络具有一定的稀疏性，从而能减轻了不同特征之间的协同效应。而且由于每次被剔除的神经元不同，所以整个网络神经元的参数也只是部分被更新，消除减弱了神经元间的联合适应性，增强了神经网络的泛化能力和鲁棒性。Dropout只在训练时使用，作为一个超参数，然而在测试集时，并不能使用。
减少欠拟合：增加迭代次数、采用验证集
3.L1和L2正则先验分别服从什么分布
L1拉普拉斯分布，L2正态分布
L2 regularizer ：使得模型的解偏向于范数较小的 W，通过限制 W 范数的大小实现了对模型空间的限制，从而在一定程度上避免了 overfitting 。不过 ridge regression 并不具有产生稀疏解的能力，得到的系数仍然需要数据中的所有特征才能计算预测结果，从计算量上来说并没有得到改观。
L1 regularizer ：它的优良性质是能产生稀疏性，导致 W 中许多项变成零。 稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。
参考：https://www.cnblogs.com/heguanyou/p/7688344.html
https://www.cnblogs.com/heguanyou/p/7688344.html
那么如何判断什么时候用L1什么时候用L2呢？https://blog.csdn.net/jinping_shi/article/details/52433975


4.对于树形结构为什么不需要归一化？
对于每一个特征，在样本之间作比较。而非用特征形成一个向量互相计算。因此每一个特征是独立的，可解释性也好。

附加题：什么是数据不平衡，如何解决？
在分类任务中：不同类别的训练样本数量差异大。比如正面情感有10000条，负面情感只有5000条。如果把训练集按照原始数据的比例来构建，会赋予样本数量较多的类别更大的权重，影响预测结果。
解决方法：对样本少的分类进行数据扩充。或者将样本数量多的分类减少样本数量。

2021.4.9
1.逻辑回归相比线性回归，有何异同？
不同之处：
1.逻辑回归解决的是分类问题，线性回归解决的是回归问题，这是两者最本质的区别
2.逻辑回归中因变量是离散的，而线性回归中因变量是连续的这是两者最大的区别
3在自变量和超参数确定的情况下逻辑回归可看作广义的线性模型在因变量下服从二元分布的一个特殊情况
4.使用最小二乘法求解线性回归时我们认为因变量服从正态分布

相同之处：
1.二者在求解超参数的过程中都使用梯度下降的方法
2.二者都使用了极大似然估计对训练样本进行建模

2.回归问题常用的性能度量指标
RMSE,R2
3.分类问题常用的性能度量指标
accuricy, recall,f1
4.逻辑回归的损失函数
sigmoid
